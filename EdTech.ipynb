{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdTech\n",
    "Elias Ejo | Jun-16-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pytube import YouTube, Playlist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define folders path:\n",
    "where source and ouput files are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_folder_path = \"Dataset/nptel_ai/\"\n",
    "frame_extr_folder = os.path.join(ds_folder_path, \"Frames_Extracted\")\n",
    "feature_extr_folder = \"Features_Extracted\"\n",
    "keyframes_folder = \"keyframes\"\n",
    "assesments_folder = \"Assesments\"\n",
    "output_folder_list = [] \n",
    "video_segmentation_sec = 30\n",
    "frame_extr_per_sec = 8\n",
    "\n",
    "# Define the keypoint labels\n",
    "keypoint_labels = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder',\n",
    "                   'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist',\n",
    "                   'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "# Create a folder to store the frames extracted\n",
    "if not(os.path.exists(frame_extr_folder)):\n",
    "    os.mkdir(frame_extr_folder) \n",
    "\n",
    "# Create segment-features extraction directory if not exists\n",
    "if not(os.path.exists(feature_extr_folder)):\n",
    "    os.mkdir(feature_extr_folder) \n",
    "\n",
    "# Create segment-keyframes extraction directory if not exists\n",
    "if not(os.path.exists(keyframes_folder)):\n",
    "    os.mkdir(keyframes_folder) \n",
    "\n",
    "# Create segment assesment result storing directory if not exists\n",
    "if not(os.path.exists(assesments_folder)):\n",
    "    os.mkdir(assesments_folder) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the vidoe from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>PlayListID</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zWg7U0OEAoE</td>\n",
       "      <td>PLJ5C_6qdAvBFfF7qtFi8Pv_RK8x55jsUQ</td>\n",
       "      <td>How to Learn and Follow the Course</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       VideoID                          PlayListID  \\\n",
       "0  zWg7U0OEAoE  PLJ5C_6qdAvBFfF7qtFi8Pv_RK8x55jsUQ   \n",
       "\n",
       "                                Title  \n",
       "0  How to Learn and Follow the Course  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load playlist dataset file using pandas\n",
    "df_nptel_ai = pd.read_csv(os.path.join(ds_folder_path, \"metadata2.csv\"))\n",
    "df_nptel_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the playlist URL\n",
    "playlist_url = f\"https://www.youtube.com/watch?v={df_nptel_ai['VideoID'][0]}&list={df_nptel_ai['PlayListID'][0]}\"\n",
    "\n",
    "# Get the video title\n",
    "video_title = f\"{df_nptel_ai['Title'][0]}\"\n",
    "\n",
    "# Construct the video file name\n",
    "video_file_name = f\"{df_nptel_ai['Title'][0]}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos in playlist: 43\n"
     ]
    }
   ],
   "source": [
    "playlist_items = []\n",
    "\n",
    "#Accessing the Playlist\n",
    "playlist = Playlist(playlist_url)\n",
    "\n",
    "#Checking the number of videos in the playlist\n",
    "print('Number of videos in playlist: %s' % len(playlist.video_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing through the playlist and appending the video urls in play_list\n",
    "for video_url in playlist.video_urls:\n",
    "    playlist_items.append(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=N-DQ8iDlH_U',\n",
       " 'https://www.youtube.com/watch?v=A_I5XAtkxiY',\n",
       " 'https://www.youtube.com/watch?v=ueFXTnG8ZC0',\n",
       " 'https://www.youtube.com/watch?v=668keuD73k4',\n",
       " 'https://www.youtube.com/watch?v=SJpd7KC18fQ',\n",
       " 'https://www.youtube.com/watch?v=r6a2WuWPBa0',\n",
       " 'https://www.youtube.com/watch?v=CdivH5kY_54',\n",
       " 'https://www.youtube.com/watch?v=feCZo_2Ql3c',\n",
       " 'https://www.youtube.com/watch?v=-CIto4pV6Zw',\n",
       " 'https://www.youtube.com/watch?v=OvhQn6HkA_0',\n",
       " 'https://www.youtube.com/watch?v=zvvUKDfwA6k',\n",
       " 'https://www.youtube.com/watch?v=9W-VlZFICCY',\n",
       " 'https://www.youtube.com/watch?v=dN2m8irACSw',\n",
       " 'https://www.youtube.com/watch?v=PPBynZnJkfM',\n",
       " 'https://www.youtube.com/watch?v=fWqx01BbeI8',\n",
       " 'https://www.youtube.com/watch?v=UOt3y50dcb0',\n",
       " 'https://www.youtube.com/watch?v=ekUGhPsFZZs',\n",
       " 'https://www.youtube.com/watch?v=LxiFzgQGDBw',\n",
       " 'https://www.youtube.com/watch?v=M-iDsvrmABw',\n",
       " 'https://www.youtube.com/watch?v=VpN3zDKjYl0',\n",
       " 'https://www.youtube.com/watch?v=xD2Zqt3HHuc',\n",
       " 'https://www.youtube.com/watch?v=AGSHaE3Ob2A',\n",
       " 'https://www.youtube.com/watch?v=CAx1lNvPOgI',\n",
       " 'https://www.youtube.com/watch?v=bzNXpXakIZ0',\n",
       " 'https://www.youtube.com/watch?v=IQIRNxfMvxA',\n",
       " 'https://www.youtube.com/watch?v=S3jNEt9SgVs',\n",
       " 'https://www.youtube.com/watch?v=nP2YpsgBXAE',\n",
       " 'https://www.youtube.com/watch?v=XciCuzhUa6E',\n",
       " 'https://www.youtube.com/watch?v=RdVTHSrlLyw',\n",
       " 'https://www.youtube.com/watch?v=fFBnIy30Uwo',\n",
       " 'https://www.youtube.com/watch?v=b-brVcf2tDo',\n",
       " 'https://www.youtube.com/watch?v=jXl55PJpxQM',\n",
       " 'https://www.youtube.com/watch?v=GzfKAAHtzX8',\n",
       " 'https://www.youtube.com/watch?v=dCikE8vA90k',\n",
       " 'https://www.youtube.com/watch?v=G53R-yechVo',\n",
       " 'https://www.youtube.com/watch?v=vDDEZH5KpDY',\n",
       " 'https://www.youtube.com/watch?v=HRKMfk7jWwc',\n",
       " 'https://www.youtube.com/watch?v=ggf1EDsQDaY',\n",
       " 'https://www.youtube.com/watch?v=DZSpWklktZc',\n",
       " 'https://www.youtube.com/watch?v=-TuZYR_ke_w',\n",
       " 'https://www.youtube.com/watch?v=OxhTzFd88sA',\n",
       " 'https://www.youtube.com/watch?v=sZwOtG0VQhY',\n",
       " 'https://www.youtube.com/watch?v=AfWpjB26iXU']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please open https://www.google.com/device and input code YJKR-PWJH\n",
      "Downloading video: How to Learn and Follow the Course\n",
      "Views : N-DQ8iDlH_U\n",
      "Views : 90051\n",
      "Description : \n",
      "Duration : 443 secs\n",
      "Ratings : None\n"
     ]
    }
   ],
   "source": [
    "#Looping through the list\n",
    "for item in playlist_items:\n",
    "  try:\n",
    "    yt = YouTube(item, use_oauth=True, allow_oauth_cache=True)\n",
    "    print('Downloading video: ' + yt.streams[0].title)\n",
    " \n",
    "    if yt.streams[0].title == video_title:\n",
    "      print(\"Views :\", yt.video_id)\n",
    "      print(\"Views :\", yt.views)\n",
    "      print(\"Description :\", yt.description)\n",
    "      # print(\"Title :\", yt.title)\n",
    "      print(\"Duration : {} secs\".format(yt.length))\n",
    "      print(\"Ratings :\", yt.rating)\n",
    "      #filters out all the files with \"mp4\" extension\n",
    "      stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "      stream.download(ds_folder_path)\n",
    "      # found the video then exist the loop\n",
    "      break\n",
    "\n",
    "  except:\n",
    "    print(\"Connection Error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Task: Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an extracted frame from a video as an image file.\n",
    "def save_extracted_frame(frame, output_folder, frame_count, vid_fps):\n",
    "    # Get current frame timestamp in seconds\n",
    "    second = (frame_count*5)*vid_fps\n",
    "    \n",
    "    if ((second % video_segmentation_sec) == 0) or (output_folder is None):\n",
    "        # Get the current segmentation number\n",
    "        seg_no = int(second//video_segmentation_sec) + 1\n",
    "\n",
    "        output_folder = os.path.join(frame_extr_folder, f\"Segment_{seg_no:d}\")\n",
    "        output_folder_list.append(output_folder)\n",
    "        \n",
    "        if not(os.path.exists(output_folder)):\n",
    "            os.mkdir(output_folder)\n",
    "    \n",
    "    # Get the current timestamp in milliseconds\n",
    "    timestamp_ms = (second*1000)\n",
    "\n",
    "    # Save the frame with the timestamp and frame count\n",
    "    output_frame_path = os.path.join(output_folder, f\"frame_{frame_count:d}_{timestamp_ms:f}_ms.jpg\")    \n",
    "    cv2.imwrite(output_frame_path, frame)\n",
    "\n",
    "    return output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that extracts frames from a video at uniform time intervals\n",
    "def extract_frames_uniform_time(video_path):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # if video is not present, show error\n",
    "    if not(cap.isOpened()):\n",
    "        print(\"Error reading file\")\n",
    "\n",
    "    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    vid_fps = 1.0/vid_fps\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    frame_count   = 0\n",
    "    output_folder = None\n",
    "\n",
    "    while frame_count < total_frames:\n",
    "        # Set the video capture to the current frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count*5)\n",
    "\n",
    "        # Read the next frame from the video capture object.\n",
    "        ret, frame = cap.read()\n",
    "        # - ret: A boolean value indicating whether a frame was successfully read.\n",
    "        #       - True if a frame was read successfully.\n",
    "        #       - False if there are no more frames to read or an error occurred.\n",
    "        # - frame: The next frame from the video, represented as a NumPy array.\n",
    "        # If frame read is successful, add it to the list\n",
    "        if ret:\n",
    "            output_folder = save_extracted_frame(frame, output_folder, frame_count, vid_fps)\n",
    "\n",
    "        # Increment the current frame by the segment frames\n",
    "        frame_count   += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a pre-trained VGG16 model for feature extraction. \n",
    "# VGG is a convolutional neural network model for image recognition. \n",
    "# VGG16 refers to a VGG model with 16 weight layers.\n",
    "# The input layer takes an image in the size of (224 x 224 x 3), \n",
    "# and the output layer is a softmax prediction on 1000 classes. \n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Get the output of the flatten layer\n",
    "flatten = Flatten()(base_model.output)\n",
    "\n",
    "# Create a new model with input and output layers\n",
    "vgg_model = tf.keras.models.Model(inputs=base_model.input, outputs=flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from an image using a pretrained VGG16 deep learning model,\n",
    "def extract_features_from_image(frame_path):\n",
    "    img = load_img(frame_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # Passes the preprocessed image through the VGG16 model to obtain the features.\n",
    "    features  = vgg_model.predict(x)\n",
    "    \n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features to a CSV file\n",
    "def save_features_df_to_csv(ds_prediction, folder_name):\n",
    "    # Define the file path to save the CSV file\n",
    "    csv_file = os.path.join(feature_extr_folder,f\"{folder_name}_features.csv\")  \n",
    "    \n",
    "    ds_prediction.to_csv(csv_file)\n",
    "\n",
    "    return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction per given segment of the video created and storing the features into a csv file\n",
    "def extract_features_and_store_to_csv(frame_folder):\n",
    "    frame_path_list = [] \n",
    "    df_prediction = None\n",
    "    \n",
    "    for filename in os.listdir(frame_folder):\n",
    "        # Generate the full path for the frame        \n",
    "        frame_path = os.path.join(frame_folder, filename)\n",
    "        # Extract features from the image\n",
    "        features = extract_features_from_image(frame_path)\n",
    "    \n",
    "        if df_prediction is None:\n",
    "            # first time so instantiate the data frame\n",
    "            df_prediction = pd.DataFrame(columns=np.arange(features.shape[1]))\n",
    "        \n",
    "        # Add a new row to df_prediction using the .loc indexer.\n",
    "        df_prediction.loc[df_prediction.shape[0]] = features[0]\n",
    "        frame_path_list.append(frame_path)\n",
    "    \n",
    "    df_prediction['Image_path'] = frame_path_list\n",
    "    \n",
    "    csv_file = save_features_df_to_csv(df_prediction, os.path.split(frame_folder)[1])\n",
    "    \n",
    "    return  csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score for a given set of data points and cluster labels.\n",
    "def calculate_silhouette_score(data, labels):\n",
    "    silhouette_avg = silhouette_score(data, labels, metric='euclidean')\n",
    "    \n",
    "    return silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the range of cluster numbers to evaluate\n",
    "def get_optimal_number_of_clusters(fea_csv_path):\n",
    "    \n",
    "    df_segment_fea = pd.read_csv(fea_csv_path)        \n",
    "    fea = df_segment_fea.drop([\"Unnamed: 0\",'Image_path'], axis=1)\n",
    "    frames = fea.to_numpy()\n",
    "\n",
    "    # Flatten frames for clustering\n",
    "    X = frames.reshape(frames.shape[0], -1)\n",
    "\n",
    "    min_clusters = 2\n",
    "    max_clusters = 10\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Initialize variables to store the maximum silhouette score and the corresponding optimal number of clusters\n",
    "    max_score = -1\n",
    "    optimal_clusters = -1\n",
    "\n",
    "    for n_clusters in range(min_clusters, max_clusters+1):\n",
    "        # Create the K-means model\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        \n",
    "        # Fit the model to the data\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Predict the clusters\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        score = calculate_silhouette_score(X, labels)\n",
    "        \n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "        # Check if the current score is higher than the maximum score so far\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            optimal_clusters = n_clusters\n",
    "            \n",
    "    return optimal_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract key frames from each cluster in a video segment\n",
    "def extract_key_frames_to_csv(segment_fea_csv_path, opt_num_clusters, segment):\n",
    "    \n",
    "    df_segment_fea = pd.read_csv(segment_fea_csv_path)\n",
    "    \n",
    "    fea = df_segment_fea.drop([\"Unnamed: 0\",'Image_path'], axis=1)\n",
    "    frames = fea.to_numpy()\n",
    "    \n",
    "    # Flatten frames for clustering\n",
    "    flattened_frames = frames.reshape(frames.shape[0], -1)\n",
    "\n",
    "    # Apply clustering algorithm\n",
    "    kmeans = KMeans(n_clusters=opt_num_clusters)    \n",
    "\n",
    "    # Predict the clusters\n",
    "    cluster_labels = kmeans.fit_predict(flattened_frames)\n",
    "\n",
    "    df_segment_fea = df_segment_fea.drop([\"Unnamed: 0\"], axis=1)\n",
    "    df_segment_fea['Cluster_Labels'] = cluster_labels\n",
    "\n",
    "    df_key_frame = pd.DataFrame(columns=[\"Segment\",\"Cluster\", \"Image_path\"])\n",
    "\n",
    "    # Iterate over clusters to identify key frames\n",
    "    for cluster_id in range(kmeans.n_clusters):\n",
    "        cluster_frames = frames[cluster_labels == cluster_id]\n",
    "        \n",
    "        # Calculate cluster centroid\n",
    "        cluster_centroid = np.mean(cluster_frames, axis=0)\n",
    "        \n",
    "        # Calculate distances between frames and centroid\n",
    "        distances = np.linalg.norm(cluster_frames - cluster_centroid, axis=1)\n",
    "        \n",
    "        # Select key frame as the frame with the minimum distance to the centroid\n",
    "        key_frame_index = np.argmin(distances)\n",
    "        key_frame_path = df_segment_fea.loc[key_frame_index].Image_path\n",
    "        df_key_frame.loc[len(df_key_frame.index)] = [segment, cluster_id, key_frame_path]\n",
    "\n",
    "        # key_frame = cluster_frames[key_frame_index]\n",
    "    key_frame_csv_file = f\"key_frames_of_cluster_segment_{segment}.csv\"\n",
    "\n",
    "    # Save key frame data to CSV file\n",
    "    df_key_frame.to_csv(os.path.join(keyframes_folder, key_frame_csv_file))\n",
    "\n",
    "    clustered_frame_csv_file = f\"Clustered_frames_segment_{segment}.csv\"\n",
    "\n",
    "    # Save clustred frame data to CSV file\n",
    "    df_segment_fea.to_csv(os.path.join(keyframes_folder, clustered_frame_csv_file))\n",
    "    \n",
    "    return key_frame_csv_file, clustered_frame_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the video file extract frames from the videos with video segmentaion as defined in the problem\n",
    "video_path = os.path.join(ds_folder_path, \"How to Learn and Follow the Course.mp4\")\n",
    "output_folder_list = []\n",
    "extract_frames_uniform_time(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features  from the frames chosen  from the video in segment wise\n",
    "extracted_features_csv_path_list = []\n",
    "\n",
    "for output_folder in output_folder_list:\n",
    "    csv_path = extract_features_and_store_to_csv(output_folder)\n",
    "    extracted_features_csv_path_list.append(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Features_Extracted\\\\Segment_10_features.csv',\n",
       " 'Features_Extracted\\\\Segment_11_features.csv',\n",
       " 'Features_Extracted\\\\Segment_12_features.csv',\n",
       " 'Features_Extracted\\\\Segment_13_features.csv',\n",
       " 'Features_Extracted\\\\Segment_14_features.csv',\n",
       " 'Features_Extracted\\\\Segment_15_features.csv',\n",
       " 'Features_Extracted\\\\Segment_1_features.csv',\n",
       " 'Features_Extracted\\\\Segment_2_features.csv',\n",
       " 'Features_Extracted\\\\Segment_3_features.csv',\n",
       " 'Features_Extracted\\\\Segment_4_features.csv',\n",
       " 'Features_Extracted\\\\Segment_5_features.csv',\n",
       " 'Features_Extracted\\\\Segment_6_features.csv',\n",
       " 'Features_Extracted\\\\Segment_7_features.csv',\n",
       " 'Features_Extracted\\\\Segment_8_features.csv',\n",
       " 'Features_Extracted\\\\Segment_9_features.csv']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_csv_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Features_Extracted\\Segment_10_features.csv 10\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_11_features.csv 11\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_12_features.csv 12\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_13_features.csv 13\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_14_features.csv 14\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_15_features.csv 15\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_1_features.csv 1\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_2_features.csv 2\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_3_features.csv 3\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_4_features.csv 4\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_5_features.csv 5\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_6_features.csv 6\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_7_features.csv 7\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_8_features.csv 8\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n",
      "\n",
      " \n",
      " Features_Extracted\\Segment_9_features.csv 9\n",
      "optimul number of cluster extracted\n",
      "key frames csv files is created\n",
      "Iteration step Completed Sucessfully\n"
     ]
    }
   ],
   "source": [
    "#Key frames extraction process per segment and store the key frames in a csv file\n",
    "segments_key_frame_csv_path = []\n",
    "segments_clustered_frame_path =[]\n",
    "\n",
    "for fea_csv_path in extracted_features_csv_path_list:\n",
    "    segment_no = fea_csv_path.split('_')[2]\n",
    "    print('\\n \\n',fea_csv_path , segment_no)\n",
    "    \n",
    "    opt_num_clusters = get_optimal_number_of_clusters(fea_csv_path)\n",
    "    print('optimul number of cluster extracted')\n",
    "    \n",
    "    # Extract key frames from a segment features\n",
    "    key_frame_path, clustered_frame_path = extract_key_frames_to_csv(fea_csv_path, opt_num_clusters,segment_no)\n",
    "    print('key frames csv files is created')\n",
    "    \n",
    "    segments_key_frame_csv_path.append(key_frame_path)\n",
    "    segments_clustered_frame_path.append(clustered_frame_path)\n",
    "    print('Iteration step Completed Sucessfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key_frames_of_cluster_segment_10.csv',\n",
       " 'key_frames_of_cluster_segment_11.csv',\n",
       " 'key_frames_of_cluster_segment_12.csv',\n",
       " 'key_frames_of_cluster_segment_13.csv',\n",
       " 'key_frames_of_cluster_segment_14.csv',\n",
       " 'key_frames_of_cluster_segment_15.csv',\n",
       " 'key_frames_of_cluster_segment_1.csv',\n",
       " 'key_frames_of_cluster_segment_2.csv',\n",
       " 'key_frames_of_cluster_segment_3.csv',\n",
       " 'key_frames_of_cluster_segment_4.csv',\n",
       " 'key_frames_of_cluster_segment_5.csv',\n",
       " 'key_frames_of_cluster_segment_6.csv',\n",
       " 'key_frames_of_cluster_segment_7.csv',\n",
       " 'key_frames_of_cluster_segment_8.csv',\n",
       " 'key_frames_of_cluster_segment_9.csv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key frames csv path listh in segment wise\n",
    "segments_key_frame_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clustered_frames_segment_10.csv',\n",
       " 'Clustered_frames_segment_11.csv',\n",
       " 'Clustered_frames_segment_12.csv',\n",
       " 'Clustered_frames_segment_13.csv',\n",
       " 'Clustered_frames_segment_14.csv',\n",
       " 'Clustered_frames_segment_15.csv',\n",
       " 'Clustered_frames_segment_1.csv',\n",
       " 'Clustered_frames_segment_2.csv',\n",
       " 'Clustered_frames_segment_3.csv',\n",
       " 'Clustered_frames_segment_4.csv',\n",
       " 'Clustered_frames_segment_5.csv',\n",
       " 'Clustered_frames_segment_6.csv',\n",
       " 'Clustered_frames_segment_7.csv',\n",
       " 'Clustered_frames_segment_8.csv',\n",
       " 'Clustered_frames_segment_9.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire frames extracted per segment is stored in this csv files \n",
    "# all the frames per segment with their features is stored in this csv files \n",
    "segments_clustered_frame_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_key_frame = pd.read_csv(os.path.join(keyframes_folder,'key_frames_of_cluster_segment_1.csv'))\n",
    "df_key_frame = pd.read_csv(os.path.join(keyframes_folder,'Clustered_frames_segment_5.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Task: Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect body keypoints using the MediaPipe\n",
    "def detect_body_keypoints(image_path):\n",
    "    #Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to RGB format (MediaPipe requires RGB images):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a pose detection object\n",
    "    mp_pose = mp.solutions.pose\n",
    "    \n",
    "    # Set a threshold for the minimum confidence score required for human detection\n",
    "    min_confidence_score = 0.75\n",
    "\n",
    "    # Initialize the pose detection model\n",
    "    pose_model = mp_pose.Pose(static_image_mode=True, min_detection_confidence=min_confidence_score)\n",
    "\n",
    "    # Process the image with pose detection\n",
    "    results = pose_model.process(image_rgb)\n",
    "\n",
    "    keypoints  = []\n",
    "\n",
    "    if results.pose_landmarks is not None:\n",
    "        # Iterate over the pose landmarks\n",
    "        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            keypoints.append(keypoint_labels[idx])\n",
    "    else:\n",
    "        keypoints  = [\"No_Key_Points\"]\n",
    "        \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform text detection on an image using the EAST text detection model\n",
    "def detect_text(image_path):\n",
    "    # the file path of the pre-trained EAST model\n",
    "    model_path = 'Model/EAST/frozen_east_text_detection.pb'\n",
    "    # Load the Frozen EAST Text Detection model:\n",
    "    net = cv2.dnn.readNet(model_path)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess the image by resizing and normalizing\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (320, 320), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input blob for the network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Forward pass through the network\n",
    "    scores, geometry = net.forward(['feature_fusion/Conv_7/Sigmoid', 'feature_fusion/concat_3'])\n",
    "\n",
    "    # Get the dimensions of the input image\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Set the minimum confidence threshold for text detection\n",
    "    min_confidence = 0.5\n",
    "\n",
    "    # Process the output scores and geometry\n",
    "    boxes = []\n",
    "    for i in range(scores.shape[2]):\n",
    "        # Extract scores and geometry data for each detection box\n",
    "        score = scores[0, 0, i, 0]\n",
    "        x1 = geometry[0, 0, i, 0] * width\n",
    "        y1 = geometry[0, 0, i, 1] * height\n",
    "        x2 = geometry[0, 0, i, 2] * width\n",
    "        y2 = geometry[0, 0, i, 3] * height\n",
    "        \n",
    "        # Filter out weak detections using a confidence threshold\n",
    "        if score < min_confidence:\n",
    "            continue\n",
    "\n",
    "        box_width  = abs(x2 - x1)\n",
    "        box_height = abs(y2 - y1)\n",
    "        boxes.append(box_width*box_height)\n",
    "\n",
    "    total_text_area = sum(boxes)\n",
    "\n",
    "    if total_text_area >0 :\n",
    "        text = \"Yes\"\n",
    "        img_area = (height*width)\n",
    "        Percent_of_text = round(total_text_area *100/img_area, 2)    \n",
    "        text_return_list = [text, Percent_of_text]\n",
    "    else:\n",
    "        text = \"No\"\n",
    "        Percent_of_text = 0   \n",
    "        text_return_list = [text, Percent_of_text]\n",
    "\n",
    "    return text_return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in an image using the Haar cascade classifier model\n",
    "def detect_faces(image_path):\n",
    "    # the file path of the pre-trained Haar cascade classifier model\n",
    "    model_path = 'model/Haarcascade/haarcascade_frontalface_default.xml'\n",
    "    \n",
    "    # creat a CascadeClassifier object\n",
    "    face_cascade = cv2.CascadeClassifier(model_path)\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # grayscale version of the input image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect faces in the input image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDetector:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        # Load the frozen inference graph\n",
    "        self.model = tf.compat.v1.GraphDef()\n",
    "        \n",
    "        with tf.io.gfile.GFile(model_path, 'rb') as f:\n",
    "            self.model.ParseFromString(f.read())\n",
    "       \n",
    "    def detect_humans(self, image_path):\n",
    "        # Load and preprocess the image\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path, target_size=(300, 300))\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Get the dimensions of the input image\n",
    "        height = 300 \n",
    "        width  = 300\n",
    "\n",
    "        # Perform inference\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            # input_tensor = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, None, None, 3), name='image_tensor')\n",
    "            input_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
    "           \n",
    "            tf.import_graph_def(self.model, input_map={'image_tensor': input_tensor})\n",
    "            output_tensor = sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "            output_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "            output_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "            scores, boxes, classes = sess.run([output_tensor, output_boxes, output_classes], feed_dict={input_tensor: image})\n",
    "\n",
    "        # Process the predictions\n",
    "        scores = scores[0]\n",
    "        boxes = boxes[0]\n",
    "        classes = classes[0]\n",
    "\n",
    "        # Score threshold\n",
    "        threshold = 0.5\n",
    "        \n",
    "        # Area of the input image\n",
    "        image_area = height*width\n",
    "        total_box_area = 0\n",
    "        human_detected = False\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "            # Class label 1 represents humans\n",
    "            if scores[i] > threshold and classes[i] == 1: \n",
    "                human_detected = True\n",
    "                box = boxes[i]\n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                box_width = xmax - xmin\n",
    "                box_height = ymax - ymin\n",
    "                \n",
    "                total_box_area += (box_width * box_height)\n",
    "\n",
    "        human_Percent = round(total_box_area*100/image_area, 2)\n",
    "        \n",
    "        return human_detected, human_Percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file path of the pre-trained  frozen TensorFlow model\n",
    "frozen_model_path = 'Model/Frozen/frozen_inference_graph.pb'\n",
    "\n",
    "frozen_model = HumanDetector(model_path=frozen_model_path)\n",
    "\n",
    "# human detetction model and function are defined here\n",
    "def human_and_interaction_assessment_detection(image_path):\n",
    "    \n",
    "    human_detected, human_Percent = frozen_model.detect_humans(image_path)\n",
    "\n",
    "    if human_detected:\n",
    "        person = \"Yes\"\n",
    "        \n",
    "        # body keypoints detection in the frame\n",
    "        body_key_points = detect_body_keypoints(image_path)\n",
    "        \n",
    "        # text detection in the frame\n",
    "        text, text_Percent = detect_text(image_path)\n",
    "        \n",
    "        # faces detection in the frame\n",
    "        faces = detect_faces(image_path)\n",
    "        \n",
    "        face = \"Yes\" if len(faces) > 0 else \"No\"\n",
    "        tool = \"Board\" if human_Percent > 10 else \"Slide\"\n",
    "\n",
    "        if (human_Percent > 10):\n",
    "            assesment = \"Interacting with Students\" if len(faces) > 0 else \"Using Board\"\n",
    "        else:\n",
    "            assesment = \"Using Slide\"            \n",
    "        \n",
    "        return_list = [image_path, text, text_Percent, person, human_Percent, face, tool, assesment, body_key_points]\n",
    "    else:\n",
    "        #text detection in the frame\n",
    "        text, text_Percent = detect_text(image_path)\n",
    "\n",
    "        if (text is not None):\n",
    "            tool = \"Slide\"\n",
    "            assesment = \"Using Slide\"\n",
    "        else:\n",
    "            tool = \"Null\"\n",
    "            assesment = \"Null\"\n",
    "        \n",
    "        return_list = [image_path, text, text_Percent, \"No\", 0, \"No\",  tool, assesment, \"No_key_points\"]\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame assessment code to be included in the main code \n",
    "# assessing the frames based on the objects detected in the image\n",
    "Assessment_csv_path_list = []\n",
    "n_segmts = 0 \n",
    "seg_num_cls = []\n",
    "seg_cls_key_frame_timestamp = []\n",
    "  \n",
    "for key_frame_csv_file in segments_key_frame_csv_path:\n",
    "    \n",
    "    # Load segments keyframes .csv file\n",
    "    df_key_frame = pd.read_csv(os.path.join(keyframes_folder, key_frame_csv_file))\n",
    "\n",
    "    # video segment number\n",
    "    seg_number = int(key_frame_csv_file.split('_')[-1].split('.')[0])\n",
    "    \n",
    "    n_segmts = n_segmts +1\n",
    " \n",
    "    df_final = pd.DataFrame(columns=['Segment_No', 'Label', 'Image_Path', 'Text', 'Text_%', 'Person', 'Human_%', \"Face\", 'Body_key_points','Teaching Aid',\"Assessment\"])\n",
    "      \n",
    "    # Counting the number of clusters per segments\n",
    "    num_cls_per_seg = df_key_frame.shape[0] \n",
    "    \n",
    "    seg_num_cls.append({'seg_{}: {}'.format(seg_number, num_cls_per_seg)})\n",
    "    \n",
    "    #iterator over cluster\n",
    "    cluster_count = 1 \n",
    "    \n",
    "    for image_path in df_key_frame['Image_path'].values:\n",
    "        \n",
    "        timestamp_ms = image_path.split('_')[-2]\n",
    "        frame_count  = image_path.split('_')[-3]\n",
    "        sckfts = f\"seg_{seg_number}_cls_{cluster_count}_frame_{frame_count}_timestamp_{timestamp_ms}_ms\"\n",
    "        \n",
    "        seg_cls_key_frame_timestamp.append(sckfts)\n",
    "\n",
    "        cluster_count += 1\n",
    "        \n",
    "        rtrn_list = human_and_interaction_assessment_detection(image_path)\n",
    "        \n",
    "        clust_label = df_key_frame[df_key_frame['Image_path']==image_path].Cluster_Labels.values[0]\n",
    "        \n",
    "        rtrn_list.insert(0, clust_label)\n",
    "        rtrn_list.insert(0, seg_number)\n",
    "        df_final.loc[df_final.shape[0]] = rtrn_list\n",
    "    \n",
    "    asmnt_file_path = os.path.join(assesments_folder,f\"segment_{seg_number}_assment.csv\")\n",
    "    Assessment_csv_path_list.append(asmnt_file_path)\n",
    "    df_final.to_csv(asmnt_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
