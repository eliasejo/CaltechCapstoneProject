# Caltech: Video Quality Assessment Capstone Project

1. Introduction: The purpose of this project is to assess the quality of E-Learning videos available on YouTube. The project involves the use of various techniques and models to analyze different aspects of the videos, including instructor presence, visual aids, and object detection. The project aims to identify key factors that contribute to engaging and effective video content. 
2. Data Collection: To gather data, a playlist library was utilized to download the E-Learning videos from YouTube. The downloaded videos served as the dataset for further analysis. 
3. Feature Extraction with VGG16: A pre-trained VGG16 model was employed for feature extraction from the videos. These features served as a basis for subsequent analysis and clustering.
4. Clustering with KMeans: To group similar videos together, the KMeans algorithm was applied to the extracted features. This allowed for the identification of clusters based on shared visual attributes. 
5. Body Keypoint Detection with MediaPipe: MediaPipe, a powerful framework, was utilized to detect body keypoints in the E-Learning videos. By identifying the positions of key body joints, such as elbows, wrists, and knees, it became possible to assess the instructor's body language and movements. This analysis provided insights into the instructor's effectiveness in conveying concepts and engaging with the audience. 
6. Text Detection with EAST Model: The EAST text detection model was employed to detect and analyze text elements within the E-Learning videos. By identifying text regions, the model facilitated an assessment of the quality and relevance of textual information displayed on slides or whiteboards. 
7. Face Detection with Haar Cascade Classifier: To assess the presence and engagement of instructors, a Haar cascade classifier model was utilized to detect faces in the video frames. 
8. Human Detection with Pre-Trained TensorFlow Model: A pre-trained TensorFlow model was used to detect human figures within the videos. This analysis aimed to evaluate the instructor's movements and interactions within the video frames. 
9. Sentiment Analysis on Video Comments: The project also incorporated sentiment analysis of video comments. The comment dataset generated from this task was deployed in Amazon SageMaker, a powerful machine learning platform, to perform sentiment analysis and determine the sentiment (positive, negative, neutral) of viewer comments. 
10. Conclusion: Through the integration of various techniques and models, this project successfully assessed the quality of E-Learning videos available on YouTube. By analyzing instructor presence, visual aids, and object detection, valuable insights were obtained regarding engaging and effective video content.
